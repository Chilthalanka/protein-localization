from typing import Tuple

import h5py
import torch
from Bio import SeqIO
from torch.utils.data import Dataset


class EmbeddingsLocalizationDataset(Dataset):
    """
    Dataset of protein embeddings and the corresponding subcellular localization label.
    """

    def __init__(self, embeddings_path: str, remapped_sequences: str, max_length: int = float('inf'),
                 transform=lambda x: x) -> None:
        """Create dataset.
        Args:
            embeddings_path: embeddings_path: path to .hdf5 .h5 file with embeddings as generated by the bio_embeddings pipeline
                https://github.com/sacdallago/bio_embeddings. Can either be a file of reduced fixed length embeddings or of
                variable length embeddings.
            remapped_sequences: remapped_sequences_file.fasta as generated by bio_embeddings where the ids in the
                annotations are the keys for the .h5 file in the embeddings path
            transform: Pytorch torchvision transforms that should be applied to each sample
            max_length: bigger sequences wont be taken into the dataset
        """
        super().__init__()
        self.transform = transform
        self.embeddings_file = h5py.File(embeddings_path, 'r')
        self.id_localization_solubility_list = []
        for record in SeqIO.parse(open(remapped_sequences), 'fasta'):
            localization = record.description.split(' ')[2].split('-')[0]
            solubility = record.description.split(' ')[2].split('-')[-1]
            if len(record.seq) <= max_length:
                self.id_localization_solubility_list.append(
                    {'id': record.id, 'localization': localization, 'solubility': solubility})

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """retrieve single sample from the dataset

        Args:
            index: index of sample to retrieve

        Returns:
            embedding: either a one dimensional Tensor [embedding_size] if the provided embeddings_path is of reduced
            embeddings or [length_of_sequence, embeddings_size] if the h5 file contains non reduced embeddings
            localization: localization in the format specified by the given transform.
        """
        id_localization_solubility = self.id_localization_solubility_list[index]
        embedding = self.embeddings_file[id_localization_solubility['id']][:]

        embedding, localization, solubility = self.transform(
            (embedding, id_localization_solubility['localization'], id_localization_solubility['solubility']))

        return embedding, localization, solubility

    def __len__(self) -> int:
        return len(self.id_localization_solubility_list)
