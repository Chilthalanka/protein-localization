experiment_name: 'attention_02_dropout'

num_epochs: 5000
batch_size: 20
lrate: 1.0e-4
log_iterations: 5

# Paths to Data
train_embeddings: 'data/embeddings/train.h5'
train_remapping: 'data/embeddings/train_remapped.fasta'
val_embeddings: 'data/embeddings/val.h5'
val_remapping: 'data/embeddings/val_remapped.fasta'
test_embeddings: 'data/embeddings/test.h5'
test_remapping: 'data/embeddings/test_remapped.fasta'

# Model parameters
model_type: 'attention'
hidden_dim: 32
num_hidden_layers: 0
dropout: 0.2
max_length: 6000